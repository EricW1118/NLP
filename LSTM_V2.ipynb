{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EricW1118/NLP_Res/blob/main/LSTM_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4c2c427",
      "metadata": {
        "id": "e4c2c427"
      },
      "outputs": [],
      "source": [
        "# Install libs needed\n",
        "!pip3 install torch \n",
        "!pip3 install torchvision\n",
        "!pip3 install torchtext\n",
        "!pip3 install pandas\n",
        "!pip3 install nltk\n",
        "!pip3 install -U scikit-learn scipy matplotlib\n",
        "!pip3 install spacy\n",
        "!pip3 install transformers\n",
        "!pip3 install seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32e80369",
      "metadata": {
        "id": "32e80369"
      },
      "outputs": [],
      "source": [
        "# Import libs\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import re\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "import string\n",
        "import seaborn as sns\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from collections import Counter\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download wordnet for lemmatization\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0o1LRjbii_A",
        "outputId": "648015e3-b2d4-425c-ade9-231cb33f0781"
      },
      "id": "O0o1LRjbii_A",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the stopwords from nltk\n",
        "nltk.download('stopwords')\n",
        "ntst = stopwords.words('english')\n",
        "print(ntst)\n",
        "print(len(ntst))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JLr8JFXimrd",
        "outputId": "214aeb05-24dd-43e1-9fec-35ef9519aabf"
      },
      "id": "4JLr8JFXimrd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "179\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import stopwords from spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "print(STOP_WORDS)\n",
        "print(len(STOP_WORDS))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etGu9_RAis90",
        "outputId": "322afb59-76ef-4353-e995-b485e2488d56"
      },
      "id": "etGu9_RAis90",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'well', 'it', 'top', 'thereupon', 'see', 'either', 'sometimes', 'thereby', 'not', 'in', 'than', 'using', 'much', 'without', 'less', 'during', 'rather', \"n't\", 'herself', 'once', 'been', '’d', 'who', 'however', 'mine', 'about', 'get', 'meanwhile', 'becomes', 'three', 'sometime', 'those', 'now', 'down', 'nowhere', 'be', 'or', 'himself', '‘d', 'beyond', 'of', 'throughout', 'make', 'still', 'some', 'being', 'per', 'although', 'namely', 'was', '‘s', 'itself', 'over', 'few', 'all', 'besides', 'last', 'wherein', 'these', '’ll', 'back', 'ten', 'latterly', 'otherwise', 'with', 'elsewhere', 'had', 'almost', 'empty', 'but', 'really', 'own', 'above', 'thereafter', 'by', 'somehow', 'yours', 'ever', \"'d\", 'hereupon', 'thence', 'since', 'has', 'every', 'before', 'whose', 'give', 'nevertheless', 'what', 'most', '‘m', 'beside', 'did', 'moreover', 'eleven', 'only', 'amongst', 'often', 'me', 'fifteen', 'fifty', 'many', 'does', 'hence', 'here', 'each', 'off', 'hereby', 'out', 'mostly', 'always', 'also', 'noone', 'indeed', 'will', 'someone', 'hundred', 'whoever', 'you', 'used', 'to', 'twelve', 'and', 'perhaps', 'from', 'our', 'can', 'anything', 'become', 'that', 'eight', 'already', 'for', 'yourself', 'another', 'formerly', '‘ll', 'first', 'others', 'least', 'under', 'done', 'please', 'am', 'front', 'yourselves', 'towards', 'made', 'both', 'upon', 'is', 'more', 'we', 'could', 'various', '’s', 'ourselves', 'as', 'else', 'keep', 'nothing', 'among', 'third', 'at', 'whenever', 'serious', 'except', '‘re', 'he', 'us', 'amount', 'latter', 'together', 'such', 'onto', 'while', 'cannot', 'the', 'though', 'very', 'through', 'when', 'whole', 'whereupon', 'put', 'which', 'so', 'whatever', 'therein', 'seems', 'just', 'ca', 'your', 'one', 'around', 'hers', '‘ve', 'then', 'myself', '’re', 'beforehand', 'four', 'full', 'anywhere', 'hereafter', 'against', \"'m\", 'whereafter', 'anyone', 'an', 'thru', 'toward', 'whither', 'no', 'why', 'via', 'do', 'may', 'ours', 'there', \"'re\", 'below', 'would', 're', 'after', 'show', 'him', 'six', 'across', 'themselves', 'bottom', 'my', 'forty', 'anyhow', 'herein', 'part', 'within', 'unless', 'any', 'enough', 'its', 'seeming', 'along', 'none', 'due', 'her', 'whereas', \"'ve\", 'their', 'became', \"'s\", 'if', '’ve', 'same', 'into', 'never', 'doing', 'n’t', 'everything', 'even', 'quite', 'whereby', 'on', 'something', 'former', \"'ll\", 'several', 'further', '’m', 'again', 'nine', 'up', 'yet', 'n‘t', 'thus', 'his', 'they', 'where', 'too', 'regarding', 'take', 'how', 'must', 'afterwards', 'name', 'because', 'them', 'this', 'a', 'whether', 'next', 'might', 'five', 'say', 'becoming', 'until', 'whence', 'are', 'nobody', 'wherever', 'side', 'somewhere', 'have', 'seem', 'two', 'i', 'anyway', 'nor', 'behind', 'other', 'therefore', 'whom', 'seemed', 'everywhere', 'neither', 'were', 'move', 'should', 'between', 'she', 'call', 'go', 'sixty', 'everyone', 'twenty', 'alone'}\n",
            "326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fac96aef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fac96aef",
        "outputId": "2b19a747-c7df-4144-82ab-dd374deb599a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'well', 'it', 'top', 'he', 'thereupon', 'see', 'us', 'either', 'sometimes', 'amount', 'thereby', 'latter', 'together', 'haven', 'such', 'onto', \"hadn't\", 'while', 'in', 'not', 'than', 'using', 'cannot', 'the', 'much', 'though', 'without', 'very', 'less', \"hasn't\", 'during', 'rather', \"n't\", 'through', 'herself', 'when', 'once', 'whole', 'whereupon', 'put', 'been', '’d', 'who', 'which', \"mightn't\", 'so', 'however', 'mine', 'about', 'get', 'whatever', 'hasn', 'therein', 'meanwhile', 'becomes', 'three', 'seems', 'just', 'ca', 'sometime', 'your', 'those', 'one', 'now', 'down', 't', 'around', \"couldn't\", 'nowhere', 'hers', '‘ve', 'be', 'or', 'then', 'myself', 'himself', 'theirs', '’re', 'beforehand', '‘d', \"you'll\", 'of', 'beyond', 'four', 'throughout', 'full', 'anywhere', 'make', 'hereafter', \"you're\", 'against', 'still', 'some', \"'m\", 'being', 'per', 'whereafter', 'anyone', 'an', 'thru', 'although', 'namely', 'was', '‘s', 'itself', 'toward', 'whither', 'mustn', 'no', 'why', 'via', 'over', 'do', \"you've\", 'may', 'ours', 'there', 'few', 'y', 'all', 'besides', 'o', 'last', 'these', 'wherein', '’ll', 'back', \"that'll\", 'ten', \"'re\", 'below', 'having', 'isn', 'latterly', 'otherwise', 'would', 're', 'with', \"won't\", 'elsewhere', 'ma', 'after', 'had', 'needn', 'shan', 'show', 'almost', 'him', 'six', \"shan't\", 'across', 'themselves', 'empty', \"weren't\", 'bottom', 'but', 'really', 'my', 'forty', 'own', 'anyhow', 'above', 'herein', 'part', 'within', \"didn't\", \"wasn't\", 'unless', 'any', 'thereafter', 'enough', 'by', 'its', 'seeming', 'somehow', 'along', 'yours', 'none', 'ever', \"'d\", 'due', 'hereupon', 'except', 'her', 'whereas', 'thence', 'since', 'has', 'every', \"'ve\", 'before', 'didn', 'their', 'became', \"'s\", 'if', 'whose', '’ve', 'same', 'into', 'give', 'never', 'nevertheless', 'doing', 'what', 'n’t', 'most', \"it's\", '‘m', 'everything', 'did', 'beside', 'moreover', 'even', 'quite', 'eleven', 'whereby', 'only', 'amongst', 'on', 'often', 'something', 'me', 'former', 'fifteen', 'd', 'fifty', \"should've\", 'many', \"'ll\", 'does', 'wouldn', 'several', 'further', '’m', 'again', 'up', 'hence', 'nine', 'yet', 'n‘t', 'here', 'hadn', \"haven't\", 'thus', 'his', 'they', 'each', 'where', 'off', 'hereby', 'too', 'out', 'mostly', 'won', 'always', 'shouldn', 'also', 'regarding', \"doesn't\", 'noone', \"wouldn't\", 'doesn', \"aren't\", 'indeed', 'will', 'how', 'take', 'must', 'afterwards', 'couldn', 'name', \"needn't\", 'because', 'someone', 'them', 'hundred', 'whoever', 'you', 'used', 'this', 'a', 'whether', 'next', 'might', 'five', 'to', 'say', 'twelve', 'until', 'and', 'becoming', 'perhaps', \"don't\", 'from', 's', 'll', 'our', 'can', 'anything', 'whence', 'become', 'are', 'that', 'eight', 'nobody', 'for', 'already', 'yourself', 'another', 'wherever', 'side', 'somewhere', \"she's\", 'formerly', '‘ll', 'have', 'm', 'first', \"isn't\", 'others', 'least', \"you'd\", 'ain', \"shouldn't\", 'seem', 'under', 'done', 'two', 'don', 'please', 'i', 'anyway', 'am', 'mightn', 'front', 'nor', 'behind', 'other', 've', 'therefore', 'yourselves', 'wasn', 'towards', 'made', 'whom', 'both', 'upon', 'seemed', 'everywhere', 'weren', 'is', 'we', 'more', 'could', 'neither', 'were', 'various', \"mustn't\", 'aren', 'should', 'move', '’s', 'between', 'ourselves', 'she', 'as', 'else', 'call', 'go', 'sixty', 'keep', 'nothing', 'everyone', 'among', 'third', 'at', 'twenty', 'whenever', 'serious', 'alone', '‘re'}\n",
            "382\n"
          ]
        }
      ],
      "source": [
        "# Merge stopwords from two libs\n",
        "stwords = set(ntst).union(STOP_WORDS)\n",
        "print(stwords)\n",
        "print(len(stwords))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be9872b4",
      "metadata": {
        "id": "be9872b4"
      },
      "outputs": [],
      "source": [
        "# Delete the stopwords\n",
        "def deleteStopwords(row):\n",
        "    text = str(row['text'])\n",
        "    tx_arr = text.split()\n",
        "    row['text'] = ' '.join( [w for w in tx_arr if w not in stwords])\n",
        "    return row\n",
        "\n",
        "# Remove punctuations including Chinses and English styles\n",
        "def remove_punctuation(row):\n",
        "    row['text'] = str(row['text']).translate(str.maketrans('', '', string.punctuation))\n",
        "    # Chinese punctuation\n",
        "    pattern = r'[。，“”‘’！？：；【】《》「」『』（）［］〔〕]'\n",
        "    # Use re.sub() to replace all matches with an empty string\n",
        "    row['text'] = re.sub(pattern, '', row['text'])\n",
        "    return row\n",
        "\n",
        "# Apply Stemming operation for each row\n",
        "stemmer = PorterStemmer()\n",
        "def stemming(row):\n",
        "    text = str(row['text'])\n",
        "    tx_arr = text.split()\n",
        "    row['text'] = ' '.join([stemmer.stem(w) for w in tx_arr])\n",
        "    return row\n",
        "\n",
        "# Apply lemmatization operation for each row\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize(row):\n",
        "    text = str(row['text'])\n",
        "    tx_arr = text.split()\n",
        "    row['text'] = ' '.join([lemmatizer.lemmatize(w) for w in tx_arr])\n",
        "    return row\n",
        "\n",
        "# After stopwords and puntuations removal, there may be some empty row in text, replace this row with np.naN\n",
        "# Then we can apply dropna to eliminate these rows\n",
        "def removeEmpty(row):\n",
        "    text = str(row['text'])\n",
        "    if text.isspace() or len(text) <= 0:\n",
        "        row['text'] = np.NaN\n",
        "    return row\n",
        "  \n",
        "# Load the csv from the path given in the parameter\n",
        "# Apply different processing methods defined above\n",
        "def load_csv(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    df.columns = [\"id\",\"title\",\"text\",\"label\"]\n",
        "    df.drop(labels=['id','title'], axis='columns', inplace=True)\n",
        "    df['text'] = df['text'].str.lower().str.replace('[^\\w\\s]',' ').str.replace('\\s\\s+', ' ')\n",
        "    df = df.apply(remove_punctuation, axis=1)\n",
        "    df = df.apply(stemming, axis=1)\n",
        "    df = df.apply(lemmatize, axis=1)\n",
        "    df = df.apply(deleteStopwords, axis=1)\n",
        "    df = df.apply(removeEmpty, axis=1)\n",
        "    df = df.dropna(subset=['text'], axis=0, how= 'all')\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd615fda",
      "metadata": {
        "id": "bd615fda"
      },
      "outputs": [],
      "source": [
        "# Only if you are using Google Colab, you need to mount the Google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5c666c3",
      "metadata": {
        "id": "e5c666c3"
      },
      "outputs": [],
      "source": [
        "# Use the function defined above to load the csv data\n",
        "# df = load_csv('WELFake_Dataset.csv')\n",
        "df = load_csv('/content/gdrive/My Drive/NLP/WELFake_Dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d793a58",
      "metadata": {
        "id": "8d793a58"
      },
      "outputs": [],
      "source": [
        "# Print the data summary \n",
        "df.info()\n",
        "\n",
        "# Print some rows of the dataset\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b48863a",
      "metadata": {
        "id": "0b48863a"
      },
      "outputs": [],
      "source": [
        "# Check if the data is balanced between all the classes\n",
        "data_count = df['label'].value_counts()\n",
        "sns.barplot(x=np.array([0,1]),y=data_count.values)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e794407",
      "metadata": {
        "id": "1e794407"
      },
      "outputs": [],
      "source": [
        "# Plot the length distribution of sentences of the dataset\n",
        "rev_len = [len(i) for i in  df['text']]\n",
        "pd.Series(rev_len).hist()\n",
        "plt.show()\n",
        "\n",
        "# Summarize the sentence length distribution\n",
        "pd.Series(rev_len).describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a7e1de2",
      "metadata": {
        "id": "7a7e1de2"
      },
      "outputs": [],
      "source": [
        "# Calculate the number of rows with lable 1 and 0\n",
        "df_zero = df[df['label']==0]\n",
        "df_one = df[df['label']==1]\n",
        "\n",
        "print('length of sentences with label zero : %4d' % (len(df_zero)))\n",
        "print('length of sentences with label one: %4d' % (len(df_one)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94889074",
      "metadata": {
        "id": "94889074"
      },
      "outputs": [],
      "source": [
        "# Calculate 80% of the number of class 0 and class 1\n",
        "# Then take the smaller one \n",
        "train_length = int(min(len(df_zero) * 0.8, len(df_one) * 0.8))\n",
        "print('length of each train set: %4d ' % (train_length))\n",
        "\n",
        "# Randomly pick the number calculated above of rows from class 1 and class 0\n",
        "# This operation makes sure we take the same number of rows from each class\n",
        "# Make sure our training dataset is balanced\n",
        "zero_train, zero_test = train_test_split(df_zero, train_size=train_length, shuffle=True, random_state=36)\n",
        "one_train, one_test = train_test_split(df_one,  train_size=train_length, shuffle=True, random_state=36)\n",
        "\n",
        "# Concate the rows selected above from each label to form the final training set\n",
        "train, test = pd.concat([zero_train, one_train]), pd.concat([zero_test, one_test])\n",
        "print('length of final train set: {}'.format(len(train)))\n",
        "print('length of final test set: {}'.format(len(test)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tockenize(x_train, y_train, x_val, y_val):\n",
        "    word_list = []\n",
        "    for sent in x_train:\n",
        "      for word in sent.split():\n",
        "        word_list.append(word)\n",
        "\n",
        "    corpus = Counter(word_list)\n",
        "    print(len(corpus))\n",
        "    # sorting on the basis of most common words\n",
        "    corpus_sorted = sorted(corpus, key=corpus.get, reverse=True)[:20000]\n",
        "    # creating a dict\n",
        "    onehot_dict = {w:i+1 for i,w in enumerate(corpus_sorted)}\n",
        "    # tockenize\n",
        "    final_list_train, final_list_test = [],[]\n",
        "\n",
        "    for sent in x_train:\n",
        "            final_list_train.append([onehot_dict[word] for word in sent.split() \n",
        "                                     if word in onehot_dict.keys()])\n",
        "    for sent in x_val:\n",
        "            final_list_test.append([onehot_dict[word] for word in sent.split() \n",
        "                                    if word in onehot_dict.keys()])\n",
        "\n",
        "    # encoded_train = [label for label in y_train]  \n",
        "    # encoded_test = [label for label in y_val] \n",
        "    \n",
        "    # print(final_list_train)\n",
        "    # return np.array(final_list_train), np.array(y_train), np.array(final_list_test), np.array(y_val), onehot_dict\n",
        "    return final_list_train, y_train, final_list_test, y_val, onehot_dict"
      ],
      "metadata": {
        "id": "ZbO2RQxXv7KM"
      },
      "id": "ZbO2RQxXv7KM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the method definve above to do the tokenization for training and test dataset\n",
        "x_train, y_train, x_test, y_test, vocab = tockenize(train['text'], train['label'], test['text'], test['label'])"
      ],
      "metadata": {
        "id": "Uz8X4w9pwU6M"
      },
      "id": "Uz8X4w9pwU6M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Length of vocabulary is {len(vocab)}')"
      ],
      "metadata": {
        "id": "Tf4PWrOjwZzr"
      },
      "id": "Tf4PWrOjwZzr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def padding_(sentences, seq_len):\n",
        "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
        "    for ii, review in enumerate(sentences):\n",
        "        if len(review) != 0:\n",
        "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
        "    return features"
      ],
      "metadata": {
        "id": "1BCNhpSBwdvY"
      },
      "id": "1BCNhpSBwdvY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_pad = padding_(x_train,800)\n",
        "x_test_pad = padding_(x_test,800)"
      ],
      "metadata": {
        "id": "78ROJwpMwe05"
      },
      "id": "78ROJwpMwe05",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train_pad)\n",
        "print(len(x_train_pad))\n",
        "\n",
        "print(x_test_pad)\n",
        "print(len(x_test_pad))"
      ],
      "metadata": {
        "id": "tMpCd4RKwoYy"
      },
      "id": "tMpCd4RKwoYy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create Tensor datasets\n",
        "train_data = TensorDataset(torch.from_numpy(np.array(x_train_pad)), torch.from_numpy(np.array(y_train)))\n",
        "valid_data = TensorDataset(torch.from_numpy(np.array(x_test_pad)), torch.from_numpy(np.array(y_test)))\n",
        "\n",
        "# dataloaders \n",
        "batch_size = 100\n",
        "\n",
        "# make sure to SHUFFLE your data\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_data, shuffle=False, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "mk1Uhb-owsG_"
      },
      "id": "mk1Uhb-owsG_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# is_cuda = not torch.cuda.is_available\n",
        "is_cuda = torch.cuda.is_available\n",
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU is not available, CPU used\")"
      ],
      "metadata": {
        "id": "IvcLJbPxKzcS"
      },
      "id": "IvcLJbPxKzcS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f802e6ce",
      "metadata": {
        "id": "f802e6ce"
      },
      "outputs": [],
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, num_classes):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
        "        self.attention = nn.Linear(hidden_size, 1)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        \n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        \n",
        "        out, _ = self.lstm(embedded, (h0, c0))\n",
        "\n",
        "        # Attention mechanism\n",
        "        attention_weights = torch.softmax(self.attention(out), dim=1)\n",
        "        attention_out = torch.sum(attention_weights * out, dim=1)\n",
        "        \n",
        "        # Extract the last timestep's output\n",
        "        # out = out[:, -1, :]\n",
        "\n",
        "        # out = self.fc(out)\n",
        "        out = self.fc(attention_out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6d79caf",
      "metadata": {
        "id": "f6d79caf"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "embedding_dim = 80\n",
        "num_classes = 2\n",
        "vocab_size = len(vocab) + 1 #extra 1 for padding\n",
        "hidden_size = 100\n",
        "num_layers = 12\n",
        "num_classes = 2\n",
        "\n",
        "# Create the model\n",
        "model = LSTMClassifier(vocab_size, embedding_dim, hidden_size, num_layers, num_classes)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(\"Total parameters:\", total_params)"
      ],
      "metadata": {
        "id": "BXkXRjW0bTNv"
      },
      "id": "BXkXRjW0bTNv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ec13e62",
      "metadata": {
        "id": "6ec13e62"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Given labels and predictions accordingly, compute the number of the right prediction\n",
        "def num_of_acc(pred,label):\n",
        "    _, predicted = torch.max(pred, dim=1)\n",
        "    # print(predicted.shape, label.shape)\n",
        "    # pred = torch.round(pred.squeeze())\n",
        "    # return torch.sum(pred == label.squeeze()).item()\n",
        "    return torch.sum(predicted == label.squeeze()).item()\n",
        "\n",
        "# Given an model and dataloader, compute the accuracy\n",
        "def acc_of(model,dataloader):\n",
        "    model.eval()\n",
        "    num_sample = 0\n",
        "    accnum = 0\n",
        "    for X, y in dataloader:\n",
        "      num_sample += len(X)\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      # print(X.shape, y.shape)\n",
        "      outputs = model(X)\n",
        "      batch_acc = num_of_acc(outputs,y)\n",
        "      accnum += batch_acc\n",
        "    return (accnum / num_sample)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()  # Assuming your classification task has multiple classes\n",
        "# criterion = nn.MultiMarginLoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.00003)  # Adjust the learning rate as needed\n",
        "num_epochs = 50  # Define the number of training epochs\n",
        "optimizer = optim.NAdam(model.parameters(), lr=0.001) \n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "    train_acc = 0.0\n",
        "    num_sample = 0\n",
        "    for i ,(X, y) in enumerate(train_loader):\n",
        "        num_sample += len(X)\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Perform forward pass\n",
        "        outputs = model(X)\n",
        "        # print(outputs.shape, y.shape)\n",
        "        batch_acc = num_of_acc(outputs,y)\n",
        "        train_acc += batch_acc\n",
        "        loss = criterion(outputs, y)\n",
        "        # Perform backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    # Print the loss for monitoring the training progress\n",
        "    val_acc = acc_of(model, valid_loader)\n",
        "    print('Epoch: %5d, Loss: %.5f, train_acc: %.5f, val_acc: %.5f' % (epoch + 1, loss.item(), train_acc / num_sample, val_acc))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics import roc_auc_score\n",
        "def plot_auc_roc(model,valid_loader, version='title', threshold=0.5):\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for inputs, labels in valid_loader :\n",
        "            inputs = inputs.to(device)\n",
        "            output = model(inputs)\n",
        "\n",
        "            output = (output > threshold).int()\n",
        "            _, output = torch.max(output, dim = 1)\n",
        "            # print(output)\n",
        "            y_pred.extend(output.tolist())\n",
        "            y_true.extend(labels.tolist())\n",
        "    \n",
        "    fpr, tpr, threshold = metrics.roc_curve(y_true, y_pred)\n",
        "    roc_auc = metrics.auc(fpr, tpr)\n",
        "    \n",
        "    print('AUC ROC : %f ' % (roc_auc))\n",
        "    print('----------------------------------------------------------')\n",
        "    \n",
        "    plt.title('AUC ROC Curve')\n",
        "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "    plt.legend(loc = 'lower right')\n",
        "    plt.plot([0, 1], [0, 1],'r--')\n",
        "    plt.xlim([0, 1])\n",
        "    plt.ylim([0, 1])\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.show()\n",
        "  \n",
        "plot_auc_roc(model, train_loader) \n",
        "\n",
        "plot_auc_roc(model, valid_loader) "
      ],
      "metadata": {
        "id": "Fpyfgm8Ctq8L"
      },
      "id": "Fpyfgm8Ctq8L",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}